{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation in LlamaIndex refers to the process of assessing the performance and quality of retrieval-augmented generation (RAG) systems. It's crucial for measuring and improving the effectiveness of LLM applications, particularly in terms of retrieval accuracy and response quality. LlamaIndex provides two main types of evaluation:\n",
    "1. Response Evaluation\n",
    "2. Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Response evaluation assesses the quality of generated answers. LlamaIndex offers several LLM-based evaluation modules, including:\n",
    "- Correctness\n",
    "- Semantic Similarity\n",
    "- Faithfulness\n",
    "- Context Relevancy\n",
    "- Answer Relevancy\n",
    "- Guideline Adherence\n",
    "\n",
    "Here's an example of how to use the Faithfulness evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# Create LLM\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "# Build index (assuming it's already created)\n",
    "vector_index = VectorStoreIndex(...)\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "# Query index\n",
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"What battles took place in New York City in the American Revolution?\"\n",
    ")\n",
    "\n",
    "# Evaluate response\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "print(str(eval_result.passing))\n",
    "\n",
    "# The evaluator checks if the response is faithful to the retrieved context,\n",
    "# helping to detect potential hallucinations or inaccuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval evaluation focuses on assessing the relevance and accuracy of retrieved sources. LlamaIndex provides metrics such as:\n",
    "- Mean Reciprocal Rank (MRR)\n",
    "- Hit Rate\n",
    "- Precision\n",
    "- Recall\n",
    "- Average Precision (AP)\n",
    "- Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# Define retriever (assuming it's already created)\n",
    "retriever = ...\n",
    "\n",
    "# Create evaluator with specific metrics\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\", \"precision\", \"recall\"], retriever=retriever\n",
    ")\n",
    "\n",
    "# Evaluate a single query\n",
    "eval_result = retriever_evaluator.evaluate(\n",
    "    query=\"Sample query\", expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")\n",
    "print(eval_result)\n",
    "\n",
    "# This evaluation compares the retrieved results against expected relevant documents,\n",
    "# providing insights into the retriever's performance across multiple metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating multiple queries, LlamaIndex offers batch evaluation capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import BatchEvalRunner\n",
    "\n",
    "# Define evaluation metrics (assuming they're already created)\n",
    "faithfulness_evaluator = ...\n",
    "relevancy_evaluator = ...\n",
    "\n",
    "# Create BatchEvalRunner\n",
    "runner = BatchEvalRunner(\n",
    "    {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "    workers=8,  # Number of parallel workers\n",
    ")\n",
    "\n",
    "# Evaluate multiple queries\n",
    "eval_results = await runner.aevaluate_queries(query_engine, queries=batch_eval_queries)\n",
    "\n",
    "# Calculate scores\n",
    "faithfulness_score = sum(\n",
    "    result.passing for result in eval_results[\"faithfulness\"]\n",
    ") / len(eval_results[\"faithfulness\"])\n",
    "relevancy_score = sum(result.passing for result in eval_results[\"relevancy\"]) / len(\n",
    "    eval_results[\"relevancy\"]\n",
    ")\n",
    "\n",
    "# These scores provide an overall assessment of the system's performance\n",
    "# across multiple queries, offering insights into faithfulness and relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex's evaluation modules allow developers to:\n",
    "- Measure the accuracy and relevance of retrieved information\n",
    "- Assess the quality and faithfulness of generated responses\n",
    "- Identify areas for improvement in RAG systems\n",
    "- Benchmark different retrieval and generation strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utmist-chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
